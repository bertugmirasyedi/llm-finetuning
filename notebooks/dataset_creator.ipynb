{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "**Question 1:** What is multicriteria decision making and why does it involve multiple criteria?\n",
      "\n",
      "**assistant<|end_header_id|>\n",
      "\n",
      "Multicriteria decision making refers to screening, prioritizing, ranking or selecting the alternatives based on human judgment from among a finite set of decision alternatives in terms of multiple usually conflicting criteria. It involves multiple criteria because corporate decision-making rarely involves a single criterion.\n"
     ]
    }
   ],
   "source": [
    "from pymupdf4llm import LlamaMarkdownReader\n",
    "\n",
    "# Define the reader object\n",
    "llama_reader = LlamaMarkdownReader()\n",
    "\n",
    "# Load the data\n",
    "indexed_chunks = llama_reader.load_data(\"../data/raw/sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three questions based on the provided text:\n",
      "\n",
      "1. What is the significance of weights in multi-criteria decision making models?\n",
      "\n",
      "assistant: Weights provide information about the relative importance of considered criteria.\n",
      "\n",
      "2. Why is using ranks to elicit weights by formulas more reliable than directly assigning weights to criteria?\n",
      "\n",
      "assistant: Because decision makers are usually more confident about the ranks of some criteria than their weights, and they can agree on ranks more easily.\n",
      "\n",
      "3. What advantages do methods that rely only on ordinal information about attribute importance have over other methods?\n",
      "\n",
      "assistant: They can be used in situations where there is time pressure, quality nature of criteria, lack of knowledge, imprecise or incomplete information, decision makerâ€™s limited attention and information processing capability.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Define the OpenAI client\n",
    "client = OpenAI(base_url=\"http://172.29.3.249:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "# Define the prompt\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"bartowski/Llama-3-8B-Instruct-Gradient-1048k-GGUF\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant that can generate questions and answers from the provided text. Present the questions and answers in a structured JSON format. Output should only be the JSON.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Generate 3 questions and answers from the provided text.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": indexed_chunks[0].text},\n",
    "    ],\n",
    "    temperature=0.2,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(completion.choices[0].message.content.strip(\"</s>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
