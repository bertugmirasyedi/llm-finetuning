{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndex Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "Processing ../data/raw/sample.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "**Question 1:** What is multicriteria decision making and why does it involve multiple criteria?\n",
      "\n",
      "**assistant<|end_header_id|>\n",
      "\n",
      "Multicriteria decision making refers to screening, prioritizing, ranking or selecting the alternatives based on human judgment from among a finite set of decision alternatives in terms of multiple usually conflicting criteria. It involves multiple criteria because corporate decision-making rarely involves a single criterion.\n"
     ]
    }
   ],
   "source": [
    "from pymupdf4llm import LlamaMarkdownReader\n",
    "\n",
    "# Define the reader object\n",
    "llama_reader = LlamaMarkdownReader()\n",
    "\n",
    "# Load the data\n",
    "indexed_chunks = llama_reader.load_data(\"../data/raw/sample.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Finetuning Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self, client: OpenAI, n_questions: int = 3):\n",
    "        self.client = client\n",
    "        self.n_questions = n_questions\n",
    "\n",
    "    def generate_questions_and_answers(self, indexed_chunks):\n",
    "        import random\n",
    "\n",
    "        responses = list()\n",
    "\n",
    "        for i in range(2):\n",
    "            # Select a random temperature between 0.0 and 0.2\n",
    "            temperature = round(random.uniform(0.0, 0.2), 2)\n",
    "\n",
    "            # Define the prompt\n",
    "            completion = self.client.chat.completions.create(\n",
    "                model=\"bartowski/Llama-3-8B-Instruct-Gradient-1048k-GGUF\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a helpful assistant that can generate questions and answers from the provided text.\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Generate {self.n_questions} questions and answers from the provided text. Output should only include the questions and answers. There must not be any other text, dash, line, or output.\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": indexed_chunks[i].text},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "            )\n",
    "\n",
    "            # Get the response\n",
    "            response = completion.choices[0].message.content.strip(\"</s>\")\n",
    "\n",
    "            # Add the response to the list\n",
    "            responses.append(response)\n",
    "\n",
    "        return responses\n",
    "\n",
    "    def convert_to_json(self, questions_and_answers):\n",
    "        import json\n",
    "\n",
    "        responses = list()\n",
    "\n",
    "        for i in range(len(questions_and_answers)):\n",
    "            # Define the prompt\n",
    "            completion = self.client.chat.completions.create(\n",
    "                model=\"bartowski/Llama-3-8B-Instruct-Gradient-1048k-GGUF\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a helpful assistant that convert the provided text to valid JSON.\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": \"Convert the provided text to valid JSON. Output should only be the valid JSON. For example: {[{'question': question1, 'answer': answer1}, {'question': question2, 'answer': answer2}, {'question': question3, 'answer': answer3}]}. There must not be any other text, dash, line, or output.\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": questions_and_answers[i]},\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "            )\n",
    "\n",
    "            # Get the response\n",
    "            response = completion.choices[0].message.content.strip(\"</s>\")\n",
    "\n",
    "            # Convert the response to a JSON object\n",
    "            response_json = json.loads(response)\n",
    "\n",
    "            # Add the response to the list\n",
    "            responses.append(response_json)\n",
    "\n",
    "        return responses\n",
    "\n",
    "    def generate_dataset(self, indexed_chunks):\n",
    "        response = self.generate_questions_and_answers(indexed_chunks)\n",
    "        response_json = self.convert_to_json(response)\n",
    "\n",
    "        return response_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'question': 'What is multicriteria decision making and how does it differ from single-criterion decision making?',\n",
       "   'answer': 'Multicriteria decision making involves screening, prioritizing, ranking or selecting alternatives based on human judgment from among a finite set of decision alternatives in terms of multiple usually conflicting criteria. This differs from single-criterion decision making which only considers one criterion.'},\n",
       "  {'question': 'What is the significance of weights in MCDM models?',\n",
       "   'answer': 'Weights play a very significant role in MCDM models as they provide information about the relative importance of the considered criteria. Different methods are developed to take these priorities into account, and using ranks to elicit weights by some formulas is more reliable than just directly assigning weights to criteria.'},\n",
       "  {'question': 'What are some examples of situations where rank ordering weight methods can be useful?',\n",
       "   'answer': 'Rank ordering weight methods can be useful in situations of time pressure, quality nature of criteria, lack of knowledge, imprecise or incomplete information, decision maker’s limited attention and information processing capability. They rely only on ordinal information about attribute importance and are simple and effective.'}],\n",
       " [{'question': 'What is the main focus of this paper on weight determination methods?',\n",
       "   'answer': 'The main focus of this paper is a comparative overview on ranking method of weight determination.'},\n",
       "  {'question': 'Why do rank ordering weighting methods provide approximations for criteria weights when only rank ordering information is known?',\n",
       "   'answer': 'Because attaching ranks to elicit weights by some formulas is more reliable than just directly assigning weights to criteria. Decision makers are usually more confident about the ranks of some criteria than their weights, and they can agree on ranks more easily.'},\n",
       "  {'question': 'What are the two practical problems that arise in weight determination methods?',\n",
       "   'answer': 'The first problem is criterion ranks: how to understand which criterion is more important than the other. The second problem is how to elicit weights from this information.'}]]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the OpenAI client\n",
    "client = OpenAI(base_url=\"http://172.29.3.249:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "# Define the LLM object\n",
    "llm = LLM(client=client)\n",
    "\n",
    "# Generate the dataset\n",
    "response = llm.generate_dataset(indexed_chunks)\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
